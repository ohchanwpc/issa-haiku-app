
# --- haiku_gpt.py (å…ˆé ­ä»˜è¿‘) ---
from __future__ import annotations
import os, re, json, time, random, logging  # â† æ—¢å­˜ã®ã¾ã¾OK
from openai import OpenAI, RateLimitError, APIStatusError  # â† æ—¢å­˜ã®ã¾ã¾OK

# ===== ãƒ­ã‚®ãƒ³ã‚°è¨­å®šï¼ˆæ—¢å­˜ã®è¨­å®šãŒã‚ã‚Œã°é‚ªé­”ã—ãªã„ï¼‰=====
_logger = logging.getLogger("haiku_gpt")
if not _logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s"))
    _logger.addHandler(_handler)
    _logger.setLevel(logging.INFO)

# ç›´è¿‘ã®APIå‘¼ã³å‡ºã—ãƒ¡ã‚¿ï¼ˆUIã‚„ãƒ‡ãƒãƒƒã‚°ã§ä½¿ã„ãŸã„æ™‚ã«å‚ç…§å¯èƒ½ï¼‰
last_call_meta: dict | None = None

def _extract_request_id(err: Exception) -> str | None:
    """
    OpenAIã®ä¾‹å¤–ã‹ã‚‰ request_id ã‚’å¯èƒ½ãªã‚‰å–å¾—ï¼ˆç„¡ã‘ã‚Œã° Noneï¼‰
    """
    try:
        resp = getattr(err, "response", None)
        # SDK ã«ã‚ˆã‚Š response.request_id or resp.headers.get("x-request-id") ãªã©
        req_id = getattr(resp, "request_id", None)
        if not req_id and hasattr(resp, "headers"):
            req_id = resp.headers.get("x-request-id")
        return req_id
    except Exception:
        return None

def _retry_call(fn, *, max_tries: int = 5, base: float = 0.8, cap: float = 8.0):
    """
    OpenAIå‘¼ã³å‡ºã—ã‚’æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‹ã‚¸ãƒƒã‚¿ãƒ¼ã§å†è©¦è¡Œã€‚
    å‘¼ã³å‡ºã—å´ã®æŒ™å‹•ã‚’å£Šã•ãªã„ãŸã‚ã€ï¼ˆæˆåŠŸæ™‚ï¼‰å…ƒã®è¿”ã‚Šå€¤ã€ï¼ˆå¤±æ•—æ™‚ï¼‰ä¾‹å¤–ã‚’å†é€å‡ºã€‚
    ãŸã ã—å‰¯ä½œç”¨ã¨ã—ã¦ã€ç›´è¿‘å‘¼ã³å‡ºã—ã®ãƒ¡ã‚¿æƒ…å ±ã‚’ `last_call_meta` ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    global last_call_meta
    tries, start = 0, time.time()
    while True:
        try:
            result = fn()
            last_call_meta = {
                "ok": True,
                "tries": tries + 1,
                "elapsed_sec": round(time.time() - start, 3),
                "error": None,
                "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
            }
            # æƒ…å ±ãƒ­ã‚°ï¼šæˆåŠŸ
            _logger.info(f"OpenAI call OK (tries={last_call_meta['tries']}, {last_call_meta['elapsed_sec']}s)")
            return result
        except (RateLimitError, APIStatusError) as e:
            tries += 1
            req_id = _extract_request_id(e)
            # è­¦å‘Šãƒ­ã‚°ï¼šãƒªãƒˆãƒ©ã‚¤äºˆå®š
            if tries < max_tries:
                sleep = min(cap, base * (2 ** (tries - 1))) + random.uniform(0, 0.4)
                _logger.warning(
                    f"{e.__class__.__name__} (req_id={req_id}) â†’ retry {tries}/{max_tries} in {sleep:.1f}s"
                )
                time.sleep(sleep)
                continue
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ï¼šæ‰“ã¡åˆ‡ã‚Š
            last_call_meta = {
                "ok": False,
                "tries": tries,
                "elapsed_sec": round(time.time() - start, 3),
                "error": {
                    "type": e.__class__.__name__,
                    "msg": str(e),
                    "request_id": req_id,
                },
                "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
            }
            _logger.error(
                f"OpenAI call failed after {tries} tries (req_id={req_id}): {e}"
            )
            raise  # â† å¤±æ•—æ™‚ã¯å¾“æ¥ã©ãŠã‚Šä¾‹å¤–ã‚’æŠ•ã’ã‚‹ï¼ˆæ—¢å­˜ã®æŒ™å‹•ã‚’ç¶­æŒï¼‰

        except Exception as e:
            # æƒ³å®šå¤–ä¾‹å¤–ï¼šå³çµ‚äº†ï¼ˆæŒ™å‹•ç¶­æŒã®ãŸã‚å†é€å‡ºï¼‰
            last_call_meta = {
                "ok": False,
                "tries": tries + 1,
                "elapsed_sec": round(time.time() - start, 3),
                "error": {
                    "type": e.__class__.__name__,
                    "msg": str(e),
                    "request_id": None,
                },
                "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
            }
            _logger.exception(f"Unexpected error during OpenAI call: {e}")
            raise

_client = None
def _get_client() -> OpenAI:
    global _client
    if _client is None:
        _client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    return _client


def call_gpt_haiku(payload: dict) -> dict:
    """æ–°ä½œä¿³å¥ï¼‹æ„è¨³ï¼‹å‚ç…§ç†ç”±ã‚’JSONã§è¿”ã™ã€‚"""
    client = _get_client()
    refs = payload.get('references', [])
    refs_numbered = "\n".join([f"{i+1}. {r.get('text','')} | å‡ºå…¸: {r.get('source','')}" for i, r in enumerate(refs)])

    system_prompt = """ã‚ãªãŸã¯ã€Œå°æ—ä¸€èŒ¶ Ã— æ–°ä½œä¿³å¥ Ã— å‚ç…§å¥é‹ç”¨ã€ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®JSONã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä½™æ–‡ãƒ»è§£èª¬ãƒ»å‰ç½®ãç¦æ­¢ï¼‰ï¼š
{
  "haiku_ja": "äº”ä¸ƒäº”ã®æ–°ä½œï¼ˆæ—¥æœ¬èªï¼‰",
  "explanation_ja": "æ—¥æœ¬èªã®æ„è¨³ãƒ»èƒŒæ™¯ï¼ˆ100-200å­—ï¼‰",
  "reasons_refs_ja": "çµè«–ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ1æ–‡ï¼‹æ”¹è¡Œï¼‹(1)(2)(3)ã‚’Markdownç®‡æ¡æ›¸ãå½¢å¼ï¼ˆ- (1) ... ã®å½¢ï¼‰ã§å‡ºåŠ›ã™ã‚‹ã€‚å½¢å¼ä¾‹:ã€ã€çµè«–ã€‘...\\n- (1) ...\\n- (2) ...\\n- (3) ...ã€"
  "references_numbered": "1. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)\\n2. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)\\n3. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)"
}
å³å®ˆäº‹é …ï¼š
- å¿…ãšå‘½ã‚’æ‡¸ã‘ã¦5éŸ³ãƒ»7éŸ³ãƒ»5éŸ³ã®æ§‹æˆã«ã™ã‚‹ï¼ˆåˆè¨ˆ17ãƒ¢ãƒ¼ãƒ©ï¼‰
- 5éŸ³ 7éŸ³ 5éŸ³ã¨ã‚¹ãƒšãƒ¼ã‚¹ã§5éŸ³ 7éŸ³ 5éŸ³ã‚’åŒºåˆ‡ã£ã¦è¡¨ç¤ºã™ã‚‹
- ãƒ¢ãƒ¼ãƒ©æ•°ã¯ã€ã‚ƒã‚…ã‚‡â†’1ãƒ¢ãƒ¼ãƒ©ã€ã£â†’1ãƒ¢ãƒ¼ãƒ©ã€ã‚“â†’1ãƒ¢ãƒ¼ãƒ©ã€é•·éŸ³ï¼ˆãƒ¼ï¼‰â†’1ãƒ¢ãƒ¼ãƒ©ã¨ã—ã¦æ•°ãˆã‚‹
- è¨˜å·ãƒ»è‹±èªãƒ»ãƒ«ãƒ“ãƒ»å¥èª­ç‚¹ãƒ»è§£èª¬ã¯å‡ºåŠ›ã—ãªã„ï¼ˆä¿³å¥ã®ã¿ï¼‰
- æ–‡æœ«ã¯åè©ãƒ»ä½“è¨€æ­¢ã‚å¯ã€åŠ©è©ã§çµ‚ã‚ã£ã¦ã‚‚è‰¯ã„
-è‡ªå·±ãƒã‚§ãƒƒã‚¯ã‚’ä»¥ä¸‹3ç‚¹å®Ÿè¡Œã—ã¦ãã ã•ã„
1) ã²ã‚‰ãŒãªã«å†…éƒ¨å¤‰æ›ã—ã¦ãƒ¢ãƒ¼ãƒ©ã‚’æ•°ãˆã‚‹ï¼ˆå‡ºåŠ›ã«ã¯è¦‹ã›ãªã„ï¼‰
2) 5-7-5ã«ãªã£ã¦ã„ãªã‘ã‚Œã°å³åº§ã«è¨€ã„æ›ãˆã¦å†ç”Ÿæˆ
3) æœ€çµ‚å‡ºåŠ›ã¯ä¿³å¥3è¡Œã®ã¿
- è‡ªç„¶ã¨æ„Ÿæƒ…ã‚’ãƒ†ãƒ¼ãƒã«ã™ã‚‹
- é¸æŠã•ã‚ŒãŸæ„Ÿæƒ…ï¼ˆplutchikï¼‰ã¨æ—¥æœ¬çš„æƒ…ç·’ï¼ˆaestheticï¼‰ã‚’å¥ã®å†…å®¹ã¾ãŸã¯èªæ„Ÿã«å¿…ãšåæ˜ ã™ã‚‹ã“ã¨ã€‚   
- å¿…ãšå‚ç…§å¥ã‚’æ´»ç”¨ã—ã¦æ–°ä½œä¿³å¥ã‚’ä½œæˆã—ã¦ãã ã•ã„
- å°æ—ä¸€èŒ¶ã‚‰ã—ã•ã§ã‚ã‚‹ã€æ“¬éŸ³èªæ´»ç”¨ã«ã¤ã„ã¦ã¯å‚ç…§å¥ã‚’ã—ã¦ç”Ÿã‹ã—ã¦ãã ã•ã„ã€‚
- å°å‹•ç‰©ã¸ã®æ„›ã«ã¤ã„ã¦ã‚‚å‚ç…§å¥ã‚’ç”Ÿã‹ã—ã¦ãã ã•ã„ã€‚
- å‚ç…§å¥ã®æ–‡æœ«ã‚’ä¼¼ã›ã¦ãã ã•ã„ã€‚
- å‚ç…§å¥(1)(2)(3)ã®å…·ä½“è¦ç´ ï¼ˆèªï¼éŸ³è±¡å¾´ï¼æ§‹å›³ï¼‰ã‚’æœ€ä½1ã¤ãšã¤åæ˜ ã™ã‚‹ã“ã¨
- JSONä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„ã€‚
- ã€Œç†ç”±ã€ã¯â€œå‚ç…§ä¿³å¥ã®é¸å®šç†ç”±â€ã€‚æ–‡é ­ã‚’ã€ã“ã®å¥ã¯ã€ã§å§‹ã‚ãªã„ã€‚
- ã€Œreasons_refs_jaã€ã¯å¿…ãšã€ã€çµè«–ã€‘ã€ã§å§‹ã‚ã€æ¬¡è¡Œä»¥é™ã«(1)(2)(3)ã‚’æ”¹è¡Œã§ä¸¦ã¹ã‚‹ã€‚
- å„(1)(2)(3)ã§ã¯ã€å‚ç…§å¥ã®å…·ä½“è¦ç´ ï¼ˆæ§‹é€ /ãƒªã‚ºãƒ /ãƒ†ãƒ¼ãƒ/èªæ„Ÿ ç­‰ï¼‰ã¨ã€æ–°ä½œã¸ã®å¤‰å¥ã‚’ç°¡æ½”ã«è¿°ã¹ã‚‹ã€‚
- **å‚ç…§å¥ã«æ“¬éŸ³èªï¼ˆç¹°ã‚Šè¿”ã—è¡¨ç¾ ğŸµï¼‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ãã®ãƒªã‚ºãƒ ã‚„éŸ¿ãã‚’æ–°ä½œã§ã©ã†æ´»ã‹ã—ãŸã‹ã‚’å¿…ãšæ›¸ãã€‚**
- ã€Œæ„è¨³ã€ã¯ä¿³å¥ã®æƒ…æ™¯ã¨æ„Ÿæƒ…ã®ã¿ï¼ˆå‚ç…§å¥ã®è©±ã¯æ›¸ã‹ãªã„ï¼‰ã€‚æ—¥æœ¬çš„æƒ…ç·’ã‚’1ã¤ä»¥ä¸Šå«ã‚ã‚‹ã€‚
- å›ºæœ‰åè©ã‚„ç¾ä»£èªéå¤šã‚’é¿ã‘ã€ä¸€èŒ¶ã‚‰ã—ã„ç´ æœ´ã•ã¨ç”Ÿå‘½ã¸ã®ã¾ãªã–ã—ã‚’é‡è¦–ã€‚

"""

    user_prompt = f"""å…¥åŠ›æ¡ä»¶ï¼š
season = {payload.get('season')}
plutchik = {payload.get('plutchik')}
aesthetic = {payload.get('aesthetic')}
keyword = {payload.get('keyword')}
experience = {payload.get('experience')}

å‚ç…§ä¿³å¥ï¼ˆå¿…ãš(1)(2)(3)ã§è¨€åŠï¼‰:
{refs_numbered}
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": system_prompt},
                  {"role": "user", "content": user_prompt}],
        temperature=0.7,
        response_format={"type": "json_object"}
    )

    content = resp.choices[0].message.content
    try:
        data = json.loads(content)
    except json.JSONDecodeError:
        s = content.strip()
        if s.startswith("```"):
            s = re.sub(r"^```[a-zA-Z]*\n?", "", s)
            s = re.sub(r"\n?```$", "", s)
        start = s.find("{"); end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            s = s[start:end+1]
        s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'")
        s = re.sub(r",(\s*[\]}])", r"\1", s)
        s = re.sub(r"'([A-Za-z0-9_]+)'\s*:", r'"\1":', s)
        s = re.sub(r":\s*'([^']*)'", lambda m: ':"{}"'.format(m.group(1).replace('"', '\\"')), s)
        try:
            data = json.loads(s)
        except json.JSONDecodeError:
            data = {"haiku_ja": "", "explanation_ja": "", "reasons_refs_ja": "", "references_numbered": refs_numbered}

    for k in ["haiku_ja", "explanation_ja", "reasons_refs_ja", "references_numbered"]:
        data.setdefault(k, "")
    return data

def generate_english_tweet_block(haiku_ja: str, explanation_ja: str) -> str:
    """æ—¥æœ¬èªä¿³å¥ï¼‹èª¬æ˜ã‹ã‚‰ X å‘ã‘è‹±èªãƒ–ãƒ­ãƒƒã‚¯ã‚’ç”Ÿæˆ"""
    client = _get_client()
    system_prompt = """ã‚ãªãŸã¯ã€Œä¿³å¥è‹±è¨³ Ã— Xï¼ˆTwitterï¼‰æŠ•ç¨¿æ•´å½¢ã€ã®å°‚é–€å®¶ã§ã™ã€‚
æ¬¡ã®4ãƒ–ãƒ­ãƒƒã‚¯ã ã‘ã‚’å‡ºåŠ›ï¼š
ğŸŒ¿ ä¿³å¥ï¼ˆæ—¥æœ¬èªï¼‰

{haiku_ja}

ğŸƒ Haiku (English)

{haiku_en_3lines}

âœ¨ Explanation

{explanation_en_short}

åˆ¶ç´„ï¼šåˆè¨ˆ280å­—ä»¥å†…ã€‚è‹±è¨³ã¯3è¡Œã€èª¬æ˜ã¯1-2æ–‡ã€‚çµµæ–‡å­—ã¯ğŸŒ¿ğŸƒâœ¨ã®ã¿ã€‚"""

    user_prompt = f"""ä¿³å¥ï¼ˆæ—¥æœ¬èªï¼‰:
{haiku_ja}

ä¿³å¥ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªã®æ„è¨³/èƒŒæ™¯ã®è¦ç‚¹ï¼‰:
{explanation_ja}
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"system","content":system_prompt},
                  {"role":"user","content":user_prompt}],
        temperature=0.5,
    )
    return resp.choices[0].message.content.strip()
