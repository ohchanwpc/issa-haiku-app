
from __future__ import annotations
import os, re, json, time, random, logging  # â† è¿½åŠ : time, random, logging
from openai import OpenAI
from openai import RateLimitError, APIStatusError

logging.basicConfig(level=logging.WARNING, format="%(asctime)s %(levelname)s %(message)s")


# SDKå·®ã‚’å¸åï¼šä¾‹å¤–ã‚¯ãƒ©ã‚¹ãŒç„¡ã„ç’°å¢ƒã§ã‚‚å‹•ãã‚ˆã†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
try:
    from openai import RateLimitError, APIStatusError
except Exception:  # å¤ã„SDKãªã©
    class RateLimitError(Exception): ...
    class APIStatusError(Exception):
        status_code: int | None = None

_client = None

def _get_client() -> OpenAI:
    global _client
    if _client is None:
        # è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤OFFã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ˜ç¤º
        _client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            max_retries=0,
            timeout=60,
        )
    return _client
key = os.getenv("OPENAI_API_KEY")
logging.warning(f"OPENAI_API_KEY head={key[:5] + 'â€¦' if key else 'None'}")



# âœ… ã“ã®ã™ãä¸‹ã«å…¥ã‚Œã‚‹ï¼ˆãŠã™ã™ã‚ä½ç½®ï¼ï¼‰
def _with_backoff(callable_fn, *, max_attempts=6, base=0.8, cap=12.0):
    last_err = None
    for attempt in range(1, max_attempts + 1):
        try:
            return callable_fn()
        except RateLimitError as e:
            last_err = e
            # 429ç³»
            logging.error(f"[TRY {attempt}/{max_attempts}] RateLimitError: {getattr(e, 'message', repr(e))}")
        except APIStatusError as e:
            last_err = e
            code = getattr(e, "status_code", None)
            # å¯èƒ½ãªã‚‰ã‚µãƒ¼ãƒå¿œç­”æœ¬æ–‡ã‚‚ãƒ­ã‚°ã«
            body_text = None
            try:
                if hasattr(e, "response") and hasattr(e.response, "text"):
                    body_text = e.response.text
            except Exception:
                pass
            logging.error(f"[TRY {attempt}/{max_attempts}] APIStatusError status={code} body={body_text}")
            # 429/5xxã®ã¿å†è©¦è¡Œã€ãã‚Œä»¥å¤–ã¯å³çµ‚äº†
            if code not in (429, 500, 502, 503, 504):
                raise
        # ã“ã“ã¾ã§æ¥ãŸã‚‰å†è©¦è¡Œ
        if attempt < max_attempts:
            sleep = min(cap, base * (2 ** (attempt - 1))) * (0.5 + random.random())
            logging.warning(f"Retrying after {sleep:.2f}s â€¦")
            time.sleep(sleep)
    # å…¨æ»…ã—ãŸã‚‰â€œè©³ç´°ä»˜ãã§â€å†Throw
    raise last_err


def call_gpt_haiku(payload: dict, max_retries: int = 5) -> dict:
    """æ–°ä½œä¿³å¥ï¼‹æ„è¨³ï¼‹å‚ç…§ç†ç”±ã‚’JSONã§è¿”ã™ã€‚"""
    client = _get_client()
    refs = payload.get('references', [])
    refs_numbered = "\n".join([f"{i+1}. {r.get('text','')} | å‡ºå…¸: {r.get('source','')}" for i, r in enumerate(refs)])


    system_prompt = """ã‚ãªãŸã¯ã€Œå°æ—ä¸€èŒ¶ Ã— æ–°ä½œä¿³å¥ Ã— å‚ç…§å¥é‹ç”¨ã€ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®JSONã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä½™æ–‡ãƒ»è§£èª¬ãƒ»å‰ç½®ãç¦æ­¢ï¼‰ï¼š
{
  "haiku_ja": "äº”ä¸ƒäº”ã®æ–°ä½œï¼ˆæ—¥æœ¬èªï¼‰",
  "explanation_ja": "æ—¥æœ¬èªã®æ„è¨³ãƒ»èƒŒæ™¯ï¼ˆ100-200å­—ï¼‰",
  "reasons_refs_ja": "çµè«–ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ1æ–‡ï¼‹æ”¹è¡Œï¼‹(1)(2)(3)ã‚’Markdownç®‡æ¡æ›¸ãå½¢å¼ï¼ˆ- (1) ... ã®å½¢ï¼‰ã§å‡ºåŠ›ã™ã‚‹ã€‚å½¢å¼ä¾‹:ã€ã€çµè«–ã€‘...\\n- (1) ...\\n- (2) ...\\n- (3) ...ã€"
  "references_numbered": "1. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)\\n2. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)\\n3. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)"
}
å³å®ˆäº‹é …ï¼š
- å¿…ãšå‘½ã‚’æ‡¸ã‘ã¦5éŸ³ãƒ»7éŸ³ãƒ»5éŸ³ã®æ§‹æˆã«ã™ã‚‹ï¼ˆåˆè¨ˆ17ãƒ¢ãƒ¼ãƒ©ï¼‰
- 5éŸ³ 7éŸ³ 5éŸ³ã¨ã‚¹ãƒšãƒ¼ã‚¹ã§5éŸ³ 7éŸ³ 5éŸ³ã‚’åŒºåˆ‡ã£ã¦è¡¨ç¤ºã™ã‚‹
- ãƒ¢ãƒ¼ãƒ©æ•°ã¯ã€ã‚ƒã‚…ã‚‡â†’1ãƒ¢ãƒ¼ãƒ©ã€ã£â†’1ãƒ¢ãƒ¼ãƒ©ã€ã‚“â†’1ãƒ¢ãƒ¼ãƒ©ã€é•·éŸ³ï¼ˆãƒ¼ï¼‰â†’1ãƒ¢ãƒ¼ãƒ©ã¨ã—ã¦æ•°ãˆã‚‹
- è¨˜å·ãƒ»è‹±èªãƒ»ãƒ«ãƒ“ãƒ»å¥èª­ç‚¹ãƒ»è§£èª¬ã¯å‡ºåŠ›ã—ãªã„ï¼ˆä¿³å¥ã®ã¿ï¼‰
- æ–‡æœ«ã¯åè©ãƒ»ä½“è¨€æ­¢ã‚å¯ã€åŠ©è©ã§çµ‚ã‚ã£ã¦ã‚‚è‰¯ã„
-è‡ªå·±ãƒã‚§ãƒƒã‚¯ã‚’ä»¥ä¸‹3ç‚¹å®Ÿè¡Œã—ã¦ãã ã•ã„
1) ã²ã‚‰ãŒãªã«å†…éƒ¨å¤‰æ›ã—ã¦ãƒ¢ãƒ¼ãƒ©ã‚’æ•°ãˆã‚‹ï¼ˆå‡ºåŠ›ã«ã¯è¦‹ã›ãªã„ï¼‰
2) 5-7-5ã«ãªã£ã¦ã„ãªã‘ã‚Œã°å³åº§ã«è¨€ã„æ›ãˆã¦å†ç”Ÿæˆ
3) æœ€çµ‚å‡ºåŠ›ã¯ä¿³å¥3è¡Œã®ã¿
- è‡ªç„¶ã¨æ„Ÿæƒ…ã‚’ãƒ†ãƒ¼ãƒã«ã™ã‚‹
- é¸æŠã•ã‚ŒãŸæ„Ÿæƒ…ï¼ˆplutchikï¼‰ã¨æ—¥æœ¬çš„æƒ…ç·’ï¼ˆaestheticï¼‰ã‚’å¥ã®å†…å®¹ã¾ãŸã¯èªæ„Ÿã«å¿…ãšåæ˜ ã™ã‚‹ã“ã¨ã€‚   
- å¿…ãšå‚ç…§å¥ã‚’æ´»ç”¨ã—ã¦æ–°ä½œä¿³å¥ã‚’ä½œæˆã—ã¦ãã ã•ã„
- å°æ—ä¸€èŒ¶ã‚‰ã—ã•ã§ã‚ã‚‹ã€æ“¬éŸ³èªæ´»ç”¨ã«ã¤ã„ã¦ã¯å‚ç…§å¥ã‚’ã—ã¦ç”Ÿã‹ã—ã¦ãã ã•ã„ã€‚
- å°å‹•ç‰©ã¸ã®æ„›ã«ã¤ã„ã¦ã‚‚å‚ç…§å¥ã‚’ç”Ÿã‹ã—ã¦ãã ã•ã„ã€‚
- å‚ç…§å¥ã®æ–‡æœ«ã‚’ä¼¼ã›ã¦ãã ã•ã„ã€‚
- å‚ç…§å¥(1)(2)(3)ã®å…·ä½“è¦ç´ ï¼ˆèªï¼éŸ³è±¡å¾´ï¼æ§‹å›³ï¼‰ã‚’æœ€ä½1ã¤ãšã¤åæ˜ ã™ã‚‹ã“ã¨
- JSONä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„ã€‚
- ã€Œç†ç”±ã€ã¯â€œå‚ç…§ä¿³å¥ã®é¸å®šç†ç”±â€ã€‚æ–‡é ­ã‚’ã€ã“ã®å¥ã¯ã€ã§å§‹ã‚ãªã„ã€‚
- ã€Œreasons_refs_jaã€ã¯å¿…ãšã€ã€çµè«–ã€‘ã€ã§å§‹ã‚ã€æ¬¡è¡Œä»¥é™ã«(1)(2)(3)ã‚’æ”¹è¡Œã§ä¸¦ã¹ã‚‹ã€‚
- å„(1)(2)(3)ã§ã¯ã€å‚ç…§å¥ã®å…·ä½“è¦ç´ ï¼ˆæ§‹é€ /ãƒªã‚ºãƒ /ãƒ†ãƒ¼ãƒ/èªæ„Ÿ ç­‰ï¼‰ã¨ã€æ–°ä½œã¸ã®å¤‰å¥ã‚’ç°¡æ½”ã«è¿°ã¹ã‚‹ã€‚
- **å‚ç…§å¥ã«æ“¬éŸ³èªï¼ˆç¹°ã‚Šè¿”ã—è¡¨ç¾ ğŸµï¼‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ãã®ãƒªã‚ºãƒ ã‚„éŸ¿ãã‚’æ–°ä½œã§ã©ã†æ´»ã‹ã—ãŸã‹ã‚’å¿…ãšæ›¸ãã€‚**
- ã€Œæ„è¨³ã€ã¯ä¿³å¥ã®æƒ…æ™¯ã¨æ„Ÿæƒ…ã®ã¿ï¼ˆå‚ç…§å¥ã®è©±ã¯æ›¸ã‹ãªã„ï¼‰ã€‚æ—¥æœ¬çš„æƒ…ç·’ã‚’1ã¤ä»¥ä¸Šå«ã‚ã‚‹ã€‚
- å›ºæœ‰åè©ã‚„ç¾ä»£èªéå¤šã‚’é¿ã‘ã€ä¸€èŒ¶ã‚‰ã—ã„ç´ æœ´ã•ã¨ç”Ÿå‘½ã¸ã®ã¾ãªã–ã—ã‚’é‡è¦–ã€‚

"""

    user_prompt = f"""å…¥åŠ›æ¡ä»¶ï¼š
season = {payload.get('season')}
plutchik = {payload.get('plutchik')}
aesthetic = {payload.get('aesthetic')}
keyword = {payload.get('keyword')}
experience = {payload.get('experience')}

å‚ç…§ä¿³å¥ï¼ˆå¿…ãš(1)(2)(3)ã§è¨€åŠï¼‰:
{refs_numbered}
"""

    # ğŸ§© APIå‘¼ã³å‡ºã—éƒ¨ï¼ˆã“ã“ã‚’æ–°ã—ãï¼‰
    def _api_call():
        return client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
            response_format={"type": "json_object"},
            max_tokens=220,
        )

    resp = _with_backoff(_api_call)
    
    content = resp.choices[0].message.content
    try:
        data = json.loads(content)
    except json.JSONDecodeError:
        s = content.strip()
        if s.startswith("```"):
            s = re.sub(r"^```[a-zA-Z]*\n?", "", s)
            s = re.sub(r"\n?```$", "", s)
        start = s.find("{"); end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            s = s[start:end+1]
        s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'")
        s = re.sub(r",(\s*[\]}])", r"\1", s)
        s = re.sub(r"'([A-Za-z0-9_]+)'\s*:", r'"\1":', s)
        s = re.sub(r":\s*'([^']*)'", lambda m: ':"{}"'.format(m.group(1).replace('"', '\\"')), s)
        try:
            data = json.loads(s)
        except json.JSONDecodeError:
            data = {"haiku_ja": "", "explanation_ja": "", "reasons_refs_ja": "", "references_numbered": refs_numbered}

    for k in ["haiku_ja", "explanation_ja", "reasons_refs_ja", "references_numbered"]:
        data.setdefault(k, "")
    return data

def generate_english_tweet_block(haiku_ja: str, explanation_ja: str) -> str:
    """æ—¥æœ¬èªä¿³å¥ï¼‹èª¬æ˜ã‹ã‚‰ X å‘ã‘è‹±èªãƒ–ãƒ­ãƒƒã‚¯ã‚’ç”Ÿæˆ"""
    client = _get_client()
    system_prompt = """ã‚ãªãŸã¯ã€Œä¿³å¥è‹±è¨³ Ã— Xï¼ˆTwitterï¼‰æŠ•ç¨¿æ•´å½¢ã€ã®å°‚é–€å®¶ã§ã™ã€‚
æ¬¡ã®4ãƒ–ãƒ­ãƒƒã‚¯ã ã‘ã‚’å‡ºåŠ›ï¼š
ğŸŒ¿ ä¿³å¥ï¼ˆæ—¥æœ¬èªï¼‰

{haiku_ja}

ğŸƒ Haiku (English)

{haiku_en_3lines}

âœ¨ Explanation

{explanation_en_short}

åˆ¶ç´„ï¼šåˆè¨ˆ280å­—ä»¥å†…ã€‚è‹±è¨³ã¯3è¡Œã€èª¬æ˜ã¯1-2æ–‡ã€‚çµµæ–‡å­—ã¯ğŸŒ¿ğŸƒâœ¨ã®ã¿ã€‚"""

    user_prompt = f"""ä¿³å¥ï¼ˆæ—¥æœ¬èªï¼‰:
{haiku_ja}

ä¿³å¥ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªã®æ„è¨³/èƒŒæ™¯ã®è¦ç‚¹ï¼‰:
{explanation_ja}
"""
client = _get_client()

def _api_call():
    return client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.5,
        max_tokens=160,  # è‹±è¨³ã¯çŸ­ã‚
    )

resp = _with_backoff(_api_call)
content = resp.choices[0].message.content.strip()
return content
