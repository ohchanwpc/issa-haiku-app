

# ===== haiku_gpt.py SAFE HEADER =====
import os
import time
import random
import logging
import json
import re



from openai import OpenAI
from openai import RateLimitError, APIStatusError
_call_seq = 0
def _bump_call_seq(tag: str):
    # å„å‘¼ã³å‡ºã—ã®é€šç•ªã‚’å‡ºã™
    import time, logging
    global _call_seq
    _call_seq += 1
    logging.warning("[CALL %04d] %s t=%.3f", _call_seq, tag, time.time())


logging.basicConfig(level=logging.WARNING, format="%(asctime)s %(levelname)s %(message)s")

_client = None

def _get_client() -> OpenAI:
    global _client
    if _client is None:
        _client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            max_retries=0,
            timeout=60,
        )
        key = os.getenv("OPENAI_API_KEY")
        logging.warning(f"OPENAI_API_KEY head={key[:5]+'â€¦' if key else 'None'}")
    return _client

def _with_backoff(callable_fn, *, max_attempts=3, base=0.8, cap=8.0):
    import time, random, logging
    last_err = None
    for attempt in range(1, max_attempts + 1):
        try:
            return callable_fn()
        except Exception as e:
            last_err = e
            # å…±é€šçš„ã« status / headers / body ã‚’å¼•ãæŠœã
            status = getattr(e, "status_code", None)
            headers = None
            body_text = None
            req_id = None
            try:
                resp = getattr(e, "response", None)
                if resp is not None:
                    headers = dict(getattr(resp, "headers", {}) or {})
                    body_text = getattr(resp, "text", None)
                    req_id = headers.get("x-request-id")
            except Exception:
                pass
            logging.error(
                "[ERR TRY %d/%d] type=%s status=%s req_id=%s headers=%s body=%s",
                attempt, max_attempts, e.__class__.__name__, status, req_id, headers, body_text
            )
            # 429/5xx ã®ã¨ãã ã‘å†è©¦è¡Œ
            if status not in (429, 500, 502, 503, 504):
                raise
        if attempt < max_attempts:
            sleep = min(cap, base * (2 ** (attempt - 1))) * (0.5 + random.random())
            logging.warning("Retrying after %.2fs â€¦", sleep)
            time.sleep(sleep)
    raise last_err


# ===== /SAFE HEADER =====
def _diag_probe_once():
    """
    å°ã•ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’1å›ã ã‘é€ã‚Šã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ˜ãƒƒãƒ€ã‚’ãƒ­ã‚°ã«å‡ºã™è¨ºæ–­ç”¨ã€‚
    æœ¬ç•ªå‡¦ç†ã«ã¯å½±éŸ¿ã—ãªã„ã€‚
    """
    import logging
    client = _get_client()
    # with_raw_response ã§ headers ã‚’å–å¾—
    raw = client.with_raw_response.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role":"system","content":"You are a helpful assistant."},
            {"role":"user","content":"Say 'pong'."},
        ],
        max_tokens=5,
        temperature=0.0,
    )
    logging.warning("[DIAG] status=%s headers=%s", raw.status_code, dict(raw.headers or {}))
    resp = raw.parse()
    logging.warning("[DIAG] content=%s", resp.choices[0].message.content.strip())



def call_gpt_haiku(payload: dict, max_retries: int = 5) -> dict:
    _bump_call_seq("call_gpt_haiku")
    logging.warning("[CALL] call_gpt_haiku invoked")
    """æ–°ä½œä¿³å¥ï¼‹æ„è¨³ï¼‹å‚ç…§ç†ç”±ã‚’JSONã§è¿”ã™ã€‚"""
    client = _get_client()
    refs = payload.get('references', [])
    refs_numbered = "\n".join([f"{i+1}. {r.get('text','')} | å‡ºå…¸: {r.get('source','')}" for i, r in enumerate(refs)])


    system_prompt = """ã‚ãªãŸã¯ã€Œå°æ—ä¸€èŒ¶ Ã— æ–°ä½œä¿³å¥ Ã— å‚ç…§å¥é‹ç”¨ã€ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®JSONã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä½™æ–‡ãƒ»è§£èª¬ãƒ»å‰ç½®ãç¦æ­¢ï¼‰ï¼š
{
  "haiku_ja": "äº”ä¸ƒäº”ã®æ–°ä½œï¼ˆæ—¥æœ¬èªï¼‰",
  "explanation_ja": "æ—¥æœ¬èªã®æ„è¨³ãƒ»èƒŒæ™¯ï¼ˆ100-200å­—ï¼‰",
  "reasons_refs_ja": "çµè«–ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ1æ–‡ï¼‹æ”¹è¡Œï¼‹(1)(2)(3)ã‚’Markdownç®‡æ¡æ›¸ãå½¢å¼ï¼ˆ- (1) ... ã®å½¢ï¼‰ã§å‡ºåŠ›ã™ã‚‹ã€‚å½¢å¼ä¾‹:ã€ã€çµè«–ã€‘...\\n- (1) ...\\n- (2) ...\\n- (3) ...ã€"
  "references_numbered": "1. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)\\n2. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)\\n3. ã€‡ã€‡ | å‡ºå…¸: â–³â–³ (å¹´)"
}
å³å®ˆäº‹é …ï¼š
- å¿…ãšå‘½ã‚’æ‡¸ã‘ã¦5éŸ³ãƒ»7éŸ³ãƒ»5éŸ³ã®æ§‹æˆã«ã™ã‚‹ï¼ˆåˆè¨ˆ17ãƒ¢ãƒ¼ãƒ©ï¼‰
- 5éŸ³ 7éŸ³ 5éŸ³ã¨ã‚¹ãƒšãƒ¼ã‚¹ã§5éŸ³ 7éŸ³ 5éŸ³ã‚’åŒºåˆ‡ã£ã¦è¡¨ç¤ºã™ã‚‹
- ãƒ¢ãƒ¼ãƒ©æ•°ã¯ã€ã‚ƒã‚…ã‚‡â†’1ãƒ¢ãƒ¼ãƒ©ã€ã£â†’1ãƒ¢ãƒ¼ãƒ©ã€ã‚“â†’1ãƒ¢ãƒ¼ãƒ©ã€é•·éŸ³ï¼ˆãƒ¼ï¼‰â†’1ãƒ¢ãƒ¼ãƒ©ã¨ã—ã¦æ•°ãˆã‚‹
- è¨˜å·ãƒ»è‹±èªãƒ»ãƒ«ãƒ“ãƒ»å¥èª­ç‚¹ãƒ»è§£èª¬ã¯å‡ºåŠ›ã—ãªã„ï¼ˆä¿³å¥ã®ã¿ï¼‰
- æ–‡æœ«ã¯åè©ãƒ»ä½“è¨€æ­¢ã‚å¯ã€åŠ©è©ã§çµ‚ã‚ã£ã¦ã‚‚è‰¯ã„
-è‡ªå·±ãƒã‚§ãƒƒã‚¯ã‚’ä»¥ä¸‹3ç‚¹å®Ÿè¡Œã—ã¦ãã ã•ã„
1) ã²ã‚‰ãŒãªã«å†…éƒ¨å¤‰æ›ã—ã¦ãƒ¢ãƒ¼ãƒ©ã‚’æ•°ãˆã‚‹ï¼ˆå‡ºåŠ›ã«ã¯è¦‹ã›ãªã„ï¼‰
2) 5-7-5ã«ãªã£ã¦ã„ãªã‘ã‚Œã°å³åº§ã«è¨€ã„æ›ãˆã¦å†ç”Ÿæˆ
3) æœ€çµ‚å‡ºåŠ›ã¯ä¿³å¥3è¡Œã®ã¿
- è‡ªç„¶ã¨æ„Ÿæƒ…ã‚’ãƒ†ãƒ¼ãƒã«ã™ã‚‹
- é¸æŠã•ã‚ŒãŸæ„Ÿæƒ…ï¼ˆplutchikï¼‰ã¨æ—¥æœ¬çš„æƒ…ç·’ï¼ˆaestheticï¼‰ã‚’å¥ã®å†…å®¹ã¾ãŸã¯èªæ„Ÿã«å¿…ãšåæ˜ ã™ã‚‹ã“ã¨ã€‚   
- å¿…ãšå‚ç…§å¥ã‚’æ´»ç”¨ã—ã¦æ–°ä½œä¿³å¥ã‚’ä½œæˆã—ã¦ãã ã•ã„
- å°æ—ä¸€èŒ¶ã‚‰ã—ã•ã§ã‚ã‚‹ã€æ“¬éŸ³èªæ´»ç”¨ã«ã¤ã„ã¦ã¯å‚ç…§å¥ã‚’ã—ã¦ç”Ÿã‹ã—ã¦ãã ã•ã„ã€‚
- å°å‹•ç‰©ã¸ã®æ„›ã«ã¤ã„ã¦ã‚‚å‚ç…§å¥ã‚’ç”Ÿã‹ã—ã¦ãã ã•ã„ã€‚
- å‚ç…§å¥ã®æ–‡æœ«ã‚’ä¼¼ã›ã¦ãã ã•ã„ã€‚
- å‚ç…§å¥(1)(2)(3)ã®å…·ä½“è¦ç´ ï¼ˆèªï¼éŸ³è±¡å¾´ï¼æ§‹å›³ï¼‰ã‚’æœ€ä½1ã¤ãšã¤åæ˜ ã™ã‚‹ã“ã¨
- JSONä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„ã€‚
- ã€Œç†ç”±ã€ã¯â€œå‚ç…§ä¿³å¥ã®é¸å®šç†ç”±â€ã€‚æ–‡é ­ã‚’ã€ã“ã®å¥ã¯ã€ã§å§‹ã‚ãªã„ã€‚
- ã€Œreasons_refs_jaã€ã¯å¿…ãšã€ã€çµè«–ã€‘ã€ã§å§‹ã‚ã€æ¬¡è¡Œä»¥é™ã«(1)(2)(3)ã‚’æ”¹è¡Œã§ä¸¦ã¹ã‚‹ã€‚
- å„(1)(2)(3)ã§ã¯ã€å‚ç…§å¥ã®å…·ä½“è¦ç´ ï¼ˆæ§‹é€ /ãƒªã‚ºãƒ /ãƒ†ãƒ¼ãƒ/èªæ„Ÿ ç­‰ï¼‰ã¨ã€æ–°ä½œã¸ã®å¤‰å¥ã‚’ç°¡æ½”ã«è¿°ã¹ã‚‹ã€‚
- **å‚ç…§å¥ã«æ“¬éŸ³èªï¼ˆç¹°ã‚Šè¿”ã—è¡¨ç¾ ğŸµï¼‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ãã®ãƒªã‚ºãƒ ã‚„éŸ¿ãã‚’æ–°ä½œã§ã©ã†æ´»ã‹ã—ãŸã‹ã‚’å¿…ãšæ›¸ãã€‚**
- ã€Œæ„è¨³ã€ã¯ä¿³å¥ã®æƒ…æ™¯ã¨æ„Ÿæƒ…ã®ã¿ï¼ˆå‚ç…§å¥ã®è©±ã¯æ›¸ã‹ãªã„ï¼‰ã€‚æ—¥æœ¬çš„æƒ…ç·’ã‚’1ã¤ä»¥ä¸Šå«ã‚ã‚‹ã€‚
- å›ºæœ‰åè©ã‚„ç¾ä»£èªéå¤šã‚’é¿ã‘ã€ä¸€èŒ¶ã‚‰ã—ã„ç´ æœ´ã•ã¨ç”Ÿå‘½ã¸ã®ã¾ãªã–ã—ã‚’é‡è¦–ã€‚

"""

    user_prompt = f"""å…¥åŠ›æ¡ä»¶ï¼š
season = {payload.get('season')}
plutchik = {payload.get('plutchik')}
aesthetic = {payload.get('aesthetic')}
keyword = {payload.get('keyword')}
experience = {payload.get('experience')}

å‚ç…§ä¿³å¥ï¼ˆå¿…ãš(1)(2)(3)ã§è¨€åŠï¼‰:
{refs_numbered}
"""
    sp_len = len(system_prompt)
    up_len = len(user_prompt)
    logging.warning("[PROMPT] system=%d chars, user=%d chars, total=%d", sp_len, up_len, sp_len+up_len)

    # ğŸ§© APIå‘¼ã³å‡ºã—éƒ¨ï¼ˆã“ã“ã‚’æ–°ã—ãï¼‰
    def _api_call():
        return client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
            response_format={"type": "json_object"},
            max_tokens=220,
        )

    resp = _with_backoff(_api_call)
    
    content = resp.choices[0].message.content
    try:
        data = json.loads(content)
    except json.JSONDecodeError:
        s = content.strip()
        if s.startswith("```"):
            s = re.sub(r"^```[a-zA-Z]*\n?", "", s)
            s = re.sub(r"\n?```$", "", s)
        start = s.find("{"); end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            s = s[start:end+1]
        s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'")
        s = re.sub(r",(\s*[\]}])", r"\1", s)
        s = re.sub(r"'([A-Za-z0-9_]+)'\s*:", r'"\1":', s)
        s = re.sub(r":\s*'([^']*)'", lambda m: ':"{}"'.format(m.group(1).replace('"', '\\"')), s)
        try:
            data = json.loads(s)
        except json.JSONDecodeError:
            data = {"haiku_ja": "", "explanation_ja": "", "reasons_refs_ja": "", "references_numbered": refs_numbered}

    for k in ["haiku_ja", "explanation_ja", "reasons_refs_ja", "references_numbered"]:
        data.setdefault(k, "")
    return data

def generate_english_tweet_block(haiku_ja: str, explanation_ja: str) -> str:
    """æ—¥æœ¬èªä¿³å¥ï¼‹èª¬æ˜ã‹ã‚‰ X å‘ã‘è‹±èªãƒ–ãƒ­ãƒƒã‚¯ã‚’ç”Ÿæˆ"""
    client = _get_client()
    system_prompt = """ã‚ãªãŸã¯ã€Œä¿³å¥è‹±è¨³ Ã— Xï¼ˆTwitterï¼‰æŠ•ç¨¿æ•´å½¢ã€ã®å°‚é–€å®¶ã§ã™ã€‚
æ¬¡ã®4ãƒ–ãƒ­ãƒƒã‚¯ã ã‘ã‚’å‡ºåŠ›ï¼š
ğŸŒ¿ ä¿³å¥ï¼ˆæ—¥æœ¬èªï¼‰

{haiku_ja}

ğŸƒ Haiku (English)

{haiku_en_3lines}

âœ¨ Explanation

{explanation_en_short}

åˆ¶ç´„ï¼šåˆè¨ˆ280å­—ä»¥å†…ã€‚è‹±è¨³ã¯3è¡Œã€èª¬æ˜ã¯1-2æ–‡ã€‚çµµæ–‡å­—ã¯ğŸŒ¿ğŸƒâœ¨ã®ã¿ã€‚"""

    user_prompt = f"""ä¿³å¥ï¼ˆæ—¥æœ¬èªï¼‰:
{haiku_ja}

ä¿³å¥ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªã®æ„è¨³/èƒŒæ™¯ã®è¦ç‚¹ï¼‰:
{explanation_ja}
"""
    client = _get_client()

    # âœ… ã“ã“ã‹ã‚‰â€œé–¢æ•°ã®ä¸­â€ã«ç½®ãã“ã¨ï¼ˆãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«å‡ºã•ãªã„ï¼‰
    def _api_call():
        return client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
            response_format={"type": "json_object"},
            max_tokens=220,
        )

    try:
        resp = _with_backoff(_api_call)
    except Exception:
        logging.exception("[call_gpt_haiku] failed")
        raise

